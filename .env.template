# Honcho Environment Variables Template
# Copy this file to .env and fill in the appropriate values
# 
# Required variables are marked with (REQUIRED)
# Optional variables have default values and can be left commented out

# =============================================================================
# Application Settings
# =============================================================================
LOG_LEVEL=INFO
FASTAPI_HOST=0.0.0.0
FASTAPI_PORT=8000

# =============================================================================
# Database Settings (REQUIRED)
# =============================================================================
# Connection URI for PostgreSQL database with pgvector support
# Must use postgresql+psycopg prefix for SQLAlchemy compatibility
DB_CONNECTION_URI=postgresql+psycopg://postgres:postgres@localhost:5432/postgres

# Optional database settings
# DB_SCHEMA=public
# DB_POOL_SIZE=10
# DB_MAX_OVERFLOW=20
# DB_POOL_TIMEOUT=30
# DB_POOL_RECYCLE=300
# DB_POOL_PRE_PING=true
# DB_POOL_USE_LIFO=true
# DB_SQL_DEBUG=false
# DB_TRACING=false

# =============================================================================
# Authentication Settings
# =============================================================================
# Whether to enable authentication (set to true for production)
AUTH_USE_AUTH=false

# JWT secret key (REQUIRED if AUTH_USE_AUTH=true)
# Generate with: python scripts/generate_jwt_secret.py
# AUTH_JWT_SECRET=your-secret-key-here

# =============================================================================
# LLM API Keys (REQUIRED for full functionality)
# =============================================================================
# OpenAI API key for embeddings
LLM_OPENAI_API_KEY=your-openai-api-key-here

# Anthropic API key for dialectic and deriver functionality
LLM_ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Google API key for summarization (if using Gemini)
# LLM_GEMINI_API_KEY=your-google-api-key-here

# Groq API key for query generation (if using Groq)
# LLM_GROQ_API_KEY=your-groq-api-key-here

# Base URL for OpenAI Compatible Requests if you want to use a different provider
# LLM_OPENAI_COMPATIBLE_BASE_URL=
# LLM_OPENAI_COMPATIBLE_API_KEY=

# =============================================================================
# LLM Configuration
# =============================================================================
# Global LLM settings
# LLM_DEFAULT_MAX_TOKENS=1000
# LLM_DEFAULT_TEMPERATURE=0.0

# Dialectic LLM settings
# LLM_DIALECTIC_PROVIDER=anthropic
# LLM_DIALECTIC_MODEL=claude-3-7-sonnet-20250219

# Query generation LLM settings
# LLM_QUERY_GENERATION_PROVIDER=groq
# LLM_QUERY_GENERATION_MODEL=llama-3.1-8b-instant

# Summarization LLM settings
# LLM_SUMMARY_PROVIDER=gemini
# LLM_SUMMARY_MODEL=gemini-2.0-flash-lite
# LLM_SUMMARY_MAX_TOKENS_SHORT=1000
# LLM_SUMMARY_MAX_TOKENS_LONG=2000

# =============================================================================
# Agent Settings
# =============================================================================
# AGENT_SEMANTIC_SEARCH_TOP_K=10
# AGENT_SEMANTIC_SEARCH_MAX_DISTANCE=0.85
# AGENT_TOM_INFERENCE_METHOD=single_prompt

# =============================================================================
# Deriver (Background Worker) Settings
# =============================================================================
# DERIVER_WORKERS=1
# DERIVER_STALE_SESSION_TIMEOUT_MINUTES=5
# DERIVER_POLLING_SLEEP_INTERVAL_SECONDS=1.0
# DERIVER_TOM_METHOD=single_prompt
# DERIVER_USER_REPRESENTATION_METHOD=long_term

# =============================================================================
# History Settings
# =============================================================================
# HISTORY_MESSAGES_PER_SHORT_SUMMARY=20
# HISTORY_MESSAGES_PER_LONG_SUMMARY=60

# =============================================================================
# Monitoring and Observability (Optional)
# =============================================================================
# Sentry error tracking
# SENTRY_ENABLED=false
# SENTRY_DSN=your-sentry-dsn-here
# SENTRY_TRACES_SAMPLE_RATE=0.1
# SENTRY_PROFILES_SAMPLE_RATE=0.1
