---
title: "LangGraph Integration"
icon: 'diagram-project'
description: "Build stateful conversational AI agents with LangGraph and Honcho"
sidebarTitle: 'LangGraph'
---

Integrate Honcho with LangGraph to build conversational AI agents that maintain memory across sessions. This guide shows you how to use Honcho's memory layer with LangGraph's orchestration.

> The full code is available on [GitHub](https://github.com/plastic-labs/discord-python-starter)

## Setup and Initialization

### Prerequisites

- Honcho API key from [app.honcho.dev](https://app.honcho.dev) (Unless running locally)
- LLM API key (This tutorial uses OpenAI)
- Python 3.9+ & Node.js 18+

### Installation

<CodeGroup>
```bash Python (uv)
uv add honcho-ai langgraph langchain-core openai python-dotenv
```

```bash Python (pip)
pip install honcho-ai langgraph langchain-core openai python-dotenv
```

```bash TypeScript (npm)
npm install @honcho-ai/sdk @langchain/langgraph openai dotenv
```

```bash TypeScript (yarn)
yarn add @honcho-ai/sdk @langchain/langgraph openai dotenv
```

```bash TypeScript (pnpm)
pnpm add @honcho-ai/sdk @langchain/langgraph openai dotenv
```
</CodeGroup>

### Environment Variables

Create a `.env` file with your API keys:

```bash
HONCHO_API_KEY=your_honcho_api_key  # Only needed for production
OPENAI_API_KEY=your_openai_key
```

<Note>
Get your Honcho API key from [app.honcho.dev](https://app.honcho.dev). For local development, you can use `environment="local"` without an API key.
</Note>

### Setup

Initialize Honcho and OpenAI clients, and define the state for your LangGraph:

<CodeGroup>
```python Python
import os
from dotenv import load_dotenv
from typing_extensions import TypedDict
from honcho import Honcho
from openai import OpenAI
from langgraph.graph import StateGraph, START, END

load_dotenv()

# Initialize Honcho
# For local development: environment="local"
# For production: environment="production", api_key=os.environ.get("HONCHO_API_KEY")
honcho = Honcho(
    environment="local"
)

# Initialize OpenAI
llm = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# Define State
class State(TypedDict):
    user_input: str
    assistant_response: str
    user_id: str
    session_id: str
```

```typescript TypeScript
import * as dotenv from "dotenv";
import { Honcho } from "@honcho-ai/sdk";
import OpenAI from "openai";
import { Annotation } from "@langchain/langgraph";
import { StateGraph, START, END } from "@langchain/langgraph";

dotenv.config();

// Initialize Honcho
// For local development: environment: "local"
// For production: environment: "production", apiKey: process.env.HONCHO_API_KEY
const honcho = new Honcho({
  environment: "local"
});

// Initialize OpenAI
const llm = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

// Define State
const StateAnnotation = Annotation.Root({
  userInput: Annotation<string>(),
  assistantResponse: Annotation<string>(),
  userId: Annotation<string>(),
  sessionId: Annotation<string>(),
});

type State = typeof StateAnnotation.State;
```
</CodeGroup>

## Create Chatbot Function

Define your chatbot logic with Honcho memory integration:

<CodeGroup>
```python Python
def chatbot(state: State):
    user_message = state["user_input"]

    # Get peers and session
    user_peer = honcho.peer(state["user_id"])
    assistant_peer = honcho.peer("assistant")
    session = honcho.session(state["session_id"])
    session.add_messages([user_peer.message(user_message)])

    # Get context in OpenAI format
    messages = session.get_context().to_openai(assistant=assistant_peer)

    # Generate response
    response = llm.chat.completions.create(
        model="gpt-4",
        messages=messages
    )
    assistant_response = response.choices[0].message.content

    # Store assistant response
    session.add_messages([assistant_peer.message(assistant_response)])

    return {"assistant_response": assistant_response}
```

```typescript TypeScript
async function chatbot(state: State) {
  const userMessage = state.userInput;

  // Get peers and session
  const userPeer = await honcho.peer(state.userId);
  const assistantPeer = await honcho.peer("assistant");
  const session = await honcho.session(state.sessionId);
  await session.addMessages([userPeer.message(userMessage)]);

  // Get context in OpenAI format
  const messages = (await session.getContext()).toOpenAI(assistantPeer);

  // Generate response
  const response = await llm.chat.completions.create({
    model: "gpt-4",
    messages: messages
  });
  const assistantResponse = response.choices[0].message.content!;

  // Store assistant response
  await session.addMessages([assistantPeer.message(assistantResponse)]);

  return { assistantResponse: assistantResponse };
}
```
</CodeGroup>

## Build LangGraph

Construct the state graph with your chatbot node:

<CodeGroup>
```python Python
graph = StateGraph(State) \
    .add_node("chatbot", chatbot) \
    .add_edge(START, "chatbot") \
    .add_edge("chatbot", END) \
    .compile()
```

```typescript TypeScript
const graph = new StateGraph(StateAnnotation)
  .addNode("chatbot", chatbot)
  .addEdge(START, "chatbot")
  .addEdge("chatbot", END)
  .compile();
```
</CodeGroup>

## Chat Loop

<CodeGroup>
```python Python
def run_conversation(user_id: str, user_input: str, session_id: str = None):
    if not session_id:
        session_id = f"session_{user_id}"

    result = graph.invoke({
        "user_input": user_input,
        "user_id": user_id,
        "session_id": session_id
    })

    return result["assistant_response"]

if __name__ == "__main__":
  print("Welcome to the AI Assistant! How can I help you today?")
  user_id = "test-user-123"
  while True:
      user_input = input("You: ")
      if user_input.lower() in ['quit', 'exit']:
          break
      response = run_conversation(user_id, user_input)
      print(f"Assistant: {response}\n")
```

```typescript TypeScript
import * as readline from "readline/promises";

async function runConversation(
  userId: string,
  userInput: string,
  sessionId?: string
): Promise<string> {
  if (!sessionId) {
    sessionId = `session_${userId}`;
  }

  const result = await graph.invoke({
    userInput: userInput,
    userId: userId,
    sessionId: sessionId,
  });

  return result.assistantResponse;
}

// Interactive chat loop
async function main() {
  console.log("Welcome to the AI Assistant! How can I help you today?");
  const userId = "test-user-123";

  const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout,
  });

  while (true) {
    const userInput = await rl.question("You: ");
    if (userInput.toLowerCase() === "quit" || userInput.toLowerCase() === "exit") {
      rl.close();
      break;
    }
    const response = await runConversation(userId, userInput);
    console.log(`Assistant: ${response}\n`);
  }
}

main();
```
</CodeGroup>

## Next Steps

Now that you have a working LangGraph integration with Honcho, you can:

- **Create Honcho tools** to fully utilize the Honcho capabilities
- **Create multi-agent systems** with multiple assistant peers
- **Explore advanced memory features** like custom peer cards and metamessages

## Related Resources

<CardGroup cols={2}>
  <Card title="Get Context" icon="messages" href="/v2/documentation/core-concepts/features/get-context">
    Learn more about retrieving and formatting conversation context
  </Card>
  <Card title="MCP Integration" icon="star-of-life" href="/v2/guides/mcp">
    Use Honcho in Claude Desktop with MCP
  </Card>
</CardGroup>
