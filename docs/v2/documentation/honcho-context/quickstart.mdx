---
title: 'Quickstart - Honcho Context'
icon: 'bolt'
sidebarTitle: 'Quickstart'
---

Implement Honcho Context in just a few steps. No signup required.

<Note>
By default, the SDK uses the demo server hosted at demo.honcho.dev. The demo server is meant for quick experimentation and the data is cleared on a regular basis. Do not use for production applications.
</Note>

## 1. Install the SDK

<CodeGroup>
```bash Python (uv)
uv add honcho-ai
```

```bash Python (pip)
pip install honcho-ai
```

```bash TypeScript (npm)
npm install @honcho-ai/sdk
```

```bash TypeScript (yarn)
yarn add @honcho-ai/sdk
```

```bash TypeScript (pnpm)
pnpm add @honcho-ai/sdk
```
</CodeGroup>

## 2. Initialize the Client

The Honcho client is the main entry point for interacting with Honcho's API. By default, it uses the demo environment and a default workspace.

<CodeGroup>
```python Python
from honcho import Honcho

# Initialize client (uses demo environment and default workspace)
honcho = Honcho()

```

```typescript TypeScript
import { Honcho } from '@honcho-ai/sdk';

// Initialize client (uses demo environment and default workspace)
const honcho = new Honcho({});

```
</CodeGroup>

## 3. Create Peers

Peers represent individual users, AI agents, or any entity in your system:

<CodeGroup>
```python Python
alice = honcho.peer("alice")
bob = honcho.peer("bob")
```

```typescript TypeScript
const alice = await honcho.peer("alice")
const bob = await honcho.peer("bob")
```
</CodeGroup>

## 4. Create a Session

Sessions can be used to organize messages amongst peers.

<CodeGroup>
```python Python
session = honcho.session("session_1", config={"deriver_disabled": True})
session.add_peers([alice, bob])
```

```typescript TypeScript
const session = await honcho.session("session_1", {config:{"deriver_disabled": true}});
await session.addPeers([alice, bob])
```
</CodeGroup>

<Note>
In Honcho, memory is a reasoning task. By default it runs inference over every message. To use Honcho just for context engineering, you can toggle off this behavior.
</Note>

## 5. Add Messages

<CodeGroup>
```python Python
session.add_messages([
    alice.message("Hi Bob, how are you?"),
    bob.message("I'm good, thank you!"),
    alice.message("What are you doing today after work?"),
    bob.message("I'm going to the gym! I've been trying to get back in shape."),
    alice.message("That's great! I should probably start exercising too."),
    bob.message("You should! I find that evening workouts help me relax."),
])
```

```typescript TypeScript
await session.addMessages([
    alice.message("Hi Bob, how are you?"),
    bob.message("I'm good, thank you!"),
    alice.message("What are you doing today after work?"),
    bob.message("I'm going to the gym! I've been trying to get back in shape."),
    alice.message("That's great! I should probably start exercising too."),
    bob.message("You should! I find that evening workouts help me relax."),
])
```
</CodeGroup>

## 6. Get Context

Curating your peer's context window is remarkably simple. The `get_context` method pulls recent messages for you based on a token limit.


<CodeGroup>
```python Python
context = session.get_context() # chain with .to_openAI() or .to_anthropic() to format for APIs
```

```typescript TypeScript
const context = await session.getContext(); // chain with .toOpenAI() or .toAnthropic() to format for APIs
```
</CodeGroup>



## 7. Putting it all together

    <CodeGroup>
```python Python
import os
from openai import OpenAI
from dotenv import load_dotenv
from honcho import Honcho

# Load environment variables (e.g., OPENAI_API_KEY in .env file)
load_dotenv()

# Create OpenAI client
openai_client = OpenAI()

# Create your Honcho client
honcho = Honcho()

# Create your peers
alice = honcho.peer("alice")
bob = honcho.peer("bob")

# Make a session, add peers to the session
session = honcho.session("session_1", config={"deriver_disabled": True})
session.add_peers([alice, bob])

# Add messages sent by your peers
session.add_messages([
    alice.message("Hi Bob, how are you?"),
    bob.message("I'm good, thank you!"),
    alice.message("What are you doing today after work?"),
    bob.message("I'm going to the gym! I've been trying to get back in shape."),
    alice.message("That's great! I should probably start exercising too."),
    bob.message("You should! I find that evening workouts help me relax."),
])

# Get context for LLM
messages = session.get_context(tokens=2000).to_openai(assistant=bob)

# Add new user message and get AI response
messages.append({
    "role": "user",
    "content": "Oh maybe I'll find them relaxing as well!"
})

response = openai_client.chat.completions.create(
    model="gpt-4o",
    messages=messages
)

# Add AI response back to session
session.add_messages([
    user.message("Oh maybe I'll find them relaxing as well!"),
    assistant.message(response.choices[0].message.content)
])

print(response.choices[0].message.content)
# Expected Output: something like "Definitely! Plus, it's a great way to end the day."
```

```typescript TypeScript
import * as dotenv from 'dotenv';
import OpenAI from 'openai';
import { Honcho } from '@honcho-ai/sdk';

// Load environment variables (e.g., OPENAI_API_KEY in .env file)
dotenv.config();

// Create OpenAI client
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Create your Honcho client
const honcho = new Honcho({});

// Main async function to handle everything
async function main() {
  try {
    // Create peers
    const alice = await honcho.peer("alice");
    const bob = await honcho.peer("bob");

    // Make a session, add peers to the session
    const session = await honcho.session("session_1", {config:{"deriver_disabled": true}});
    await session.addPeers([alice, bob]);

    // Add messages sent by your peers
    await session.addMessages([
      alice.message("Hi Bob, how are you?"),
      bob.message("I'm good, thank you!"),
      alice.message("What are you doing today after work?"),
      bob.message("I'm going to the gym! I've been trying to get back in shape."),
      alice.message("That's great! I should probably start exercising too."),
      bob.message("You should! I find that evening workouts help me relax."),
    ]);

    // Get context for LLM (await the context first, then call toOpenAI)
    const context = await session.getContext({ tokens: 2000 });
    const messages = context.toOpenAI(bob)
    // Add new user message
    messages.push({
      role: "user",
      content: "Oh maybe I'll find them relaxing as well!"
    });

    // Get AI response
    const response = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: messages,
    });

    const aiResponse = response.choices[0].message.content;

    // Add AI response back to session (user as alice, AI as bob)
    await session.addMessages([
      alice.message("Oh maybe I'll find them relaxing as well!"),
      bob.message(aiResponse!),
    ]);

    // Print the AI response
    console.log(aiResponse);
  } catch (error) {
    console.error('Error running the script:', error);
  }
}

// Run the main function
main();
// Expected Output: something like "Definitely! Plus, it's a great way to end the day."
```
</CodeGroup>

## Recap

1. We set up our connection to Honcho.
2. Created our peers.
3. Made a session and added our peers.
4. Added messages from our peers to the session.
5. Used the `get_context` method to structure a stateful request to OpenAI.

We're just scratching the surface. Choose from one of the cards below to keep building with Honcho.

<CardGroup cols={3}>
  <Card title="Get Context" icon="rocket"
    href="/v2/documentation/core-concepts/features/get-context">
    Learn more about the power and flexibility of the `get_context` method
  </Card>
  <Card title="Start Building" icon="wrench" href="https://app.honcho.dev">
    Sign up on the Honcho Platform for unlimited storage and retrieval
  </Card>
  <Card title="Honcho Memory" icon="brain" href="/v2/guides/overview">
    Leverage the advanced reasoning capabilities in Honcho
    </Card>
</CardGroup>
