---
title: "Dialectic Endpoint"
description: "An endpoint for reasoning about your users"
sidebarTitle: "Dialectic Endpoint"
icon: "message-question"
---

The Dialectic endpoint (`peer.chat()`) is the natural language interface to Honcho's reasoning. Instead of manually retrieving facts or observations, your LLM can ask questions and get synthesized answers based on all the reasoning Honcho has done about a peer. Think of it as agent-to-agent communication.

## How It Works

Honcho builds a *representation*(TODO: link to concept page) for each peer--a collection of conclusions drawn from continuous reasoning over context. The most flexible way to query representations is through the `chat()` method. Some examples:

- "What is the user's preferred communication style?"
- "Has the user mentioned any dietary restrictions?"
- "What tasks does the user struggle with?"

Honcho searches the peer's representation, retrieves relevant conclusions, and synthesizes a natural language answer. It acts like a detective reasoning over evidence to make a case--your LLM asks the question, Honcho composes an answer from its conclusions.

## Basic Usage

The simplest way to use the Dialectic endpoint is to ask a question and get a text response:

<CodeGroup>
```python Python
from honcho import Honcho

honcho = Honcho()
peer = honcho.peer("user-123")

# Ask Honcho about the peer
query = "What is the user's favorite way of completing the task?"
answer = peer.chat(query)

print(answer)
# "Based on observations, the user prefers using keyboard shortcuts..."
```

```typescript TypeScript
import { Honcho } from '@honcho-ai/sdk';

const honcho = new Honcho({});
const peer = await honcho.peer("user-123");

// Ask Honcho about the peer
const query = "What is the user's favorite way of completing the task?";
const answer = await peer.chat(query);

console.log(answer);
// "Based on observations, the user prefers using keyboard shortcuts..."
```
</CodeGroup>

The Dialectic endpoint searches through the peer's representation--all the conclusions Honcho has reasoned about them--and synthesizes a natural language answer.

## Streaming Responses

For longer answers, use streaming to get incremental responses:

<CodeGroup>
```python Python
query = "What do we know about the user?"
response_stream = peer.chat(query, stream=True)

for chunk in response_stream.iter_text():
    print(chunk, end="", flush=True)
```

```typescript TypeScript
const query = "What do we know about the user?";
const responseStream = await peer.chat(query, { stream: true });

for await (const chunk of responseStream.iter_text()) {
    process.stdout.write(chunk);
}
```
</CodeGroup>

Streaming is useful for displaying real-time responses in chat interfaces or when asking complex questions that require longer answers.

## Integration Patterns

### Dynamic Prompt Enhancement

Let your LLM decide what it needs to know, then inject that context into the next generation:

<CodeGroup>
```python Python
# Your LLM generates a query based on the conversation
llm_query = "Does the user prefer formal or casual communication?"

# Get answer from Honcho
context = peer.chat(llm_query)

# Add to your next LLM prompt
enhanced_prompt = f"""
Context about the user: {context}

User message: {user_input}

Respond appropriately based on the context.
"""
```

```typescript TypeScript
// Your LLM generates a query based on the conversation
const llmQuery = "Does the user prefer formal or casual communication?";

// Get answer from Honcho
const context = await peer.chat(llmQuery);

// Add to your next LLM prompt
const enhancedPrompt = `
Context about the user: ${context}

User message: ${userInput}

Respond appropriately based on the context.
`;
```
</CodeGroup>

### Conditional Logic

Use Dialectic responses to drive application logic:

<CodeGroup>
```python Python
# Check if user has completed onboarding
onboarding_status = peer.chat("Has the user completed the onboarding flow?")

if "yes" in onboarding_status.lower():
    # Show main interface
    pass
else:
    # Show onboarding
    pass
```

```typescript TypeScript
// Check if user has completed onboarding
const onboardingStatus = await peer.chat("Has the user completed the onboarding flow?");

if (onboardingStatus.toLowerCase().includes("yes")) {
    // Show main interface
} else {
    // Show onboarding
}
```
</CodeGroup>

### Preference Extraction

Extract specific preferences for personalization:

<CodeGroup>
```python Python
# Get multiple insights
tone = peer.chat("What tone does the user prefer in responses?")
expertise = peer.chat("What is the user's level of technical expertise?")
goals = peer.chat("What are the user's main goals or objectives?")

# Use these to configure your agent's behavior
```

```typescript TypeScript
// Get multiple insights
const tone = await peer.chat("What tone does the user prefer in responses?");
const expertise = await peer.chat("What is the user's level of technical expertise?");
const goals = await peer.chat("What are the user's main goals or objectives?");

// Use these to configure your agent's behavior
```
</CodeGroup>

## How Honcho Answers

When you call `peer.chat(query)`:

1. Honcho searches through the peer's representation - conclusions drawn from reasoning over their messages
2. Retrieves conclusions semantically relevant to your query
3. Synthesizes them into a coherent natural language answer
4. Returns the answer to your application

The *deriver* runs continuously in the background, reasoning over new messages and updating representations. The Dialectic endpoint always has access to Honcho's latest conclusions about the peer.

## Best Practices

### Ask specific questions
Instead of "Tell me about the user", ask "What communication style does the user prefer?" You'll get more actionable answers.

### Let your LLM formulate queries
The Dialectic endpoint shines when your LLM decides what it needs to know. This creates dynamic, context-aware personalization.

### Use for runtime decisions
Don't just use Dialectic for LLM prompts - use it to drive application logic, routing, and feature flags based on user behavior.

### Combine with get_context()
Use `get_context()` for conversation context and `peer.chat()` for specific insights. They complement each other.

For more ideas on using the Dialectic endpoint, see our blog post on [flexible agent communication](https://blog.plasticlabs.ai/blog/Introducing-Honcho's-Dialectic-API#how-it-works).
