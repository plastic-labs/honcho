---
title: 'Migrating from Mem0'
description: 'A guide to migrate from Mem0 to Honcho'
icon: 'arrow-right-arrow-left'
---

Transferring data from Mem0 to Honcho is straightforward. This guide covers why to switch, how to migrate your data and API comparisons.

## Why Honcho?

**Inference, Not Just Storage** - Honcho builds evolving profiles of each participant through rich logic-based reasoning. Rather than retrieving facts, Honcho infers communication styles, values, patterns, and relationships - adapting to how users interact, not just what they say.

**Superior Performance** - Higher benchmark scores (more comming)

**Competitive Pricing** -

**Multi-Peer Sessions** - Native support for group conversations with multiple users and AI agents in a single session with configurable observation settings.

## Quick Comparison

### Multi-Peer Sessions

<CodeGroup>
```python Mem0
from mem0 import MemoryClient

client = MemoryClient(api_key="your-api-key")

# Single-user focus - no group context
client.add("data", user_id="user123")
```

```python Honcho
from honcho import Honcho
from honcho.session import SessionPeerConfig

honcho = Honcho(api_key="your-api-key", environment="production")

# Create peers and session
user_a = honcho.peer("user_a")
user_b = honcho.peer("user_b")
ai_agent = honcho.peer("ai_agent")
session = honcho.session("group-chat")

# Add peers with observation config
session.add_peers([
    (user_a, SessionPeerConfig(observe_others=True, observe_me=True)),
    (user_b, SessionPeerConfig(observe_others=True, observe_me=True)),
    (ai_agent, SessionPeerConfig(observe_others=False, observe_me=False)),
])

# Native group conversation support
session.add_messages([
    user_a.message("Let's focus on revenue"),
    user_b.message("I agree with the revenue goal"),
    ai_agent.message("Revenue strategy recommended...")
])
# Honcho tracks individual preferences + group dynamics
```
</CodeGroup>

### Rich Behavioral Inference

<CodeGroup>
```python Mem0
from mem0 import MemoryClient

client = MemoryClient(api_key="your-api-key")

# Simple fact retrieval
client.add("User prefers short emails", user_id="user123")
results = client.search("communication style", user_id="user123")
# Returns exact text matches
```

```python Honcho
from honcho import Honcho

honcho = Honcho(api_key="your-api-key", environment="production")

# Rich behavioral inference
user = honcho.peer("user123")
session = honcho.session("current-session")
session.add_peers([user])

session.add_messages([user.message("I prefer quick updates")])
# Automatically infers: "Values efficiency, prefers concise communication"

# Query what Honcho has learned about the user
insights = user.chat("How should I communicate with this user?")
```
</CodeGroup>

### Concept Mapping

| Mem0 | Honcho | Notes |
|------|--------|-------|
| `user_id` | `peer` | Supports both users and AI agents |
| `client.add()` | `session.add_messages()` | Session-scoped message storage |
| `client.search()` | `peer.search()` | Semantic search across messages |

### Honcho-Only Features

These capabilities have no direct Mem0 equivalent:

| Honcho | Description |
|--------|-------------|
| `peer.chat()` | Query the representation of the peer, what it knows about its peers, nuanced preferences or general questions. Based on learned patterns, not just stored facts. |
| `peer.card()` | Get stable biographical facts about a peer (name, preferences, background) - the "who they are" profile |
| `session.working_rep()` | Get cached psychological analysis for a peer in this session (mental state, intentions, conclusions) - "what they're thinking now" |
| `session.get_context()` | Retrieve optimized context with auto-summaries for LLM prompts |
| `session.get_summaries()` | Access auto-generated short and long session summaries |
| `SessionPeerConfig` | Configure observation settings (who learns about whom) |

## Migration Steps

### 1. Export from Mem0

Use Mem0's [export API](https://docs.mem0.ai/cookbooks/essentials/exporting-memories):

```python
from mem0 import MemoryClient

client = MemoryClient(api_key="your-mem0-api-key")
memories = client.get_all(filters={"user_id": "user123"}, page_size=100)
```

### 2. Install and Initialize Honcho

<CodeGroup>
```bash pip
pip install honcho-ai
```

```bash uv
uv add honcho-ai
```

```bash npm
npm install @honcho-ai/sdk
```
</CodeGroup>

<CodeGroup>
```python Python
from honcho import Honcho

honcho = Honcho(
    api_key="your-api-key",
    environment="production"
)
```

```typescript TypeScript
import { Honcho } from '@honcho-ai/sdk';

const honcho = new Honcho({
    apiKey: process.env.HONCHO_API_KEY!,
    environment: "production"
});
```
</CodeGroup>

<Info>Get your API key at [app.honcho.dev/api-keys](https://app.honcho.dev/api-keys)</Info>

### 3. Import Your Data

```python
for memory in memories:
    user_id = memory['filters']['user_id']
    peer = honcho.peer(user_id)
    session = honcho.session(f"imported-{user_id}")
    session.add_peers([peer])

    # Add message history
    if 'messages' in memory:
        session.add_messages([
            peer.message(msg['content']) for msg in memory['messages']
        ])
    elif 'memory' in memory:
        session.add_messages([peer.message(memory['memory'])])
```

### 4. Update Application Code

```python
# Before: Mem0
client.add(messages, user_id="user123")
results = client.search("query", user_id="user123")

# After: Honcho (direct migration)
user = honcho.peer("user123")
session = honcho.session("current-session")
session.add_peers([user])
session.add_messages([user.message("content")])
results = user.search("query")  # Semantic search across messages

# Upgrade: Use Honcho's inference-powered dialectic API
insights = user.chat("What communication style does this user prefer?")
```

## Advanced Features

Honcho enables capabilities not available in Mem0:

```python
# Multi-peer sessions
alice = honcho.peer("alice")
bob = honcho.peer("bob")
session = honcho.session("group-chat")
session.add_peers([alice, bob])

# Get cached working representation for a peer in this session
working_rep = session.working_rep(alice)

# Get biographical peer card
peer_card = alice.card()

# Session context with summaries for LLM prompts
context = session.get_context()

# Inference-powered queries (dialectic API)
insights = alice.chat("What communication style does Alice prefer?")
```

## Next Steps

<CardGroup cols={3}>
  <Card title="Architecture" icon="rocket" href="/v2/documentation/core-concepts/architecture">
    Understand peers and sessions
  </Card>
  <Card title="Dialectic API" icon="brain" href="/v2/documentation/core-concepts/features/dialectic-endpoint">
    Inference responses
  </Card>
  <Card title="Guides" icon="book" href="/v2/guides/overview">
    Integration examples
  </Card>
</CardGroup>

Questions? Join our [Discord](https://discord.gg/honcho) or open an issue on [GitHub](https://github.com/plastic-labs/honcho/issues).
