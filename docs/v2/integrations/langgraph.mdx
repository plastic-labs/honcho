---
title: "LangGraph"
icon: 'diagram-project'
description: "Build a stateful conversational AI agent with LangGraph and Honcho"
sidebarTitle: 'LangGraph'
---

Integrate Honcho with LangGraph to build a conversational AI agent that maintains memory across sessions. This guide shows you how to use Honcho's memory layer with LangGraph's orchestration.

> The full code is available on [GitHub](https://github.com/plastic-labs/honcho/blob/main/examples/langgraph_integration.py)

## Installation

Install required packages:

<CodeGroup>
```bash Python (uv)
uv add honcho-ai langgraph langchain-core openai python-dotenv
```

```bash Python (pip)
pip install honcho-ai langgraph langchain-core openai python-dotenv
```

```bash TypeScript (npm)
npm install @honcho-ai/sdk @langchain/langgraph openai dotenv
```

```bash TypeScript (yarn)
yarn add @honcho-ai/sdk @langchain/langgraph openai dotenv
```

```bash TypeScript (pnpm)
pnpm add @honcho-ai/sdk @langchain/langgraph openai dotenv
```
</CodeGroup>

## Configuration

### API Keys

This tutorial uses OpenAI, but Honcho works with any LLM provider. Create a `.env` file with your API keys:

```bash
OPENAI_API_KEY=your_openai_key
```

<Note>
This example uses the Honcho demo server at https://demo.honcho.dev which runs a small instance of Honcho on the latest version. For production, get your Honcho API key at [app.honcho.dev](https://app.honcho.dev). For local development, use `environment="local"`.
</Note>

## Initialize Clients

Initialize Honcho and OpenAI clients:

<CodeGroup>
```python Python
import os
from typing import Any
from dotenv import load_dotenv
from typing_extensions import TypedDict
from honcho import Honcho
from openai import OpenAI
from langgraph.graph import StateGraph, START, END

load_dotenv()

# Initialize Honcho
honcho = Honcho()

# Initialize OpenAI
llm = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
```

```typescript TypeScript
import * as dotenv from "dotenv";
import { Honcho } from "@honcho-ai/sdk";
import OpenAI from "openai";
import { Annotation } from "@langchain/langgraph";
import { StateGraph, START, END } from "@langchain/langgraph";
import * as readline from "readline/promises";

dotenv.config();

// Initialize Honcho
const honcho = new Honcho({});

// Initialize OpenAI
const llm = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});
```
</CodeGroup>

## Define LangGraph State

Define your state schema to pass data through the graph. The state stores Honcho objects directly (`user`, `assistant`, and `session`) along with the most recent message from both the user (`user_message`) and assistant (`assistant_response`).

<CodeGroup>
```python Python
class State(TypedDict):
    user_input: str
    assistant_response: str
    user: Any  # Honcho Peer object
    assistant: Any  # Honcho Peer object
    session: Any  # Honcho Session object
```

```typescript TypeScript
const StateAnnotation = Annotation.Root({
  userInput: Annotation<string>(),
  assistantResponse: Annotation<string>(),
  user: Annotation<any>(),  // Honcho Peer object
  assistant: Annotation<any>(),  // Honcho Peer object
  session: Annotation<any>(),  // Honcho Session object
});

type State = typeof StateAnnotation.State;
```
</CodeGroup>

## Create Chatbot Function

Define your chatbot logic, using Honcho to retrieve conversation context. This function demonstrates how to use Honcho to: store messages, retrieve context, and generate responses.

<CodeGroup>
```python Python
def chatbot(state: State):
    user_message = state["user_input"]

    # Get objects from state
    user = state["user"]
    assistant = state["assistant"]
    session = state["session"]

    # Step 1: Store the user's message in the session
    # This adds it to Honcho's memory for future context retrieval
    session.add_messages([user.message(user_message)])

    # Step 2: Get context in OpenAI format
    # get_context() retrieves relevant conversation history
    # to_openai() converts it to the format expected by OpenAI's API
    messages = session.get_context().to_openai(assistant=assistant)

    # Step 3: Generate response using the context
    response = llm.chat.completions.create(
        model="gpt-5.1",
        messages=messages
    )
    assistant_response = response.choices[0].message.content

    # Step 4: Store assistant response in Honcho for future context
    session.add_messages([assistant.message(assistant_response)])

    return {"assistant_response": assistant_response}
```

```typescript TypeScript
async function chatbot(state: State) {
  const userMessage = state.userInput;

  // Get objects from state
  const user = state.user;
  const assistant = state.assistant;
  const session = state.session;

  // Step 1: Store the user's message in the session
  // This adds it to Honcho's memory for future context retrieval
  await session.addMessages([user.message(userMessage)]);

  // Step 2: Get context in OpenAI format
  // getContext() retrieves relevant conversation history and formats it
  // toOpenAI() converts it to the format expected by OpenAI's API
  const messages = (await session.getContext()).toOpenAI(assistant);

  // Step 3: Generate response using the context
  const response = await llm.chat.completions.create({
    model: "gpt-5.1",
    messages: messages
  });
  const assistantResponse = response.choices[0].message.content!;

  // Step 4: Store assistant response for future context
  await session.addMessages([assistant.message(assistantResponse)]);

  return { assistantResponse: assistantResponse };
}
```
</CodeGroup>

### Understanding get_context()

The `get_context()` method retrieves the conversation history and formats it for your LLM. It automatically:

- **Manages conversation history** - Tracks all messages and determines what's relevant
- **Respects token limits** - Stays within context window constraints without manual counting
- **Handles long conversations** - Combines recent detailed messages with summaries of older exchanges

That's it. Call `session.get_context().to_openai(assistant)` and you get properly formatted context ready for your LLM.


## Build LangGraph

Construct the state graph with your chatbot node:

<CodeGroup>
```python Python
graph = StateGraph(State) \
    .add_node("chatbot", chatbot) \
    .add_edge(START, "chatbot") \
    .add_edge("chatbot", END) \
    .compile()
```

```typescript TypeScript
const graph = new StateGraph(StateAnnotation)
  .addNode("chatbot", chatbot)
  .addEdge(START, "chatbot")
  .addEdge("chatbot", END)
  .compile();
```
</CodeGroup>

## Chat Loop

Initialize the Honcho objects once and pass them into the graph:

<CodeGroup>
```python Python
def run_conversation(user_id: str, user_input: str, session_id: str = None):
    if not session_id:
        session_id = f"session_{user_id}"

    # Initialize Honcho objects once
    user = honcho.peer(user_id)
    assistant = honcho.peer("assistant")
    session = honcho.session(session_id)

    result = graph.invoke({
        "user_input": user_input,
        "user": user,
        "assistant": assistant,
        "session": session
    })

    return result["assistant_response"]

if __name__ == "__main__":
  print("Welcome to the AI Assistant! How can I help you today?")
  user_id = "test-user-123"
  while True:
      user_input = input("You: ")
      if user_input.lower() in ['quit', 'exit']:
          break
      response = run_conversation(user_id, user_input)
      print(f"Assistant: {response}\n")
```

```typescript TypeScript
async function runConversation(
  userId: string,
  userInput: string,
  sessionId?: string
): Promise<string> {
  if (!sessionId) {
    sessionId = `session_${userId}`;
  }

  // Initialize Honcho objects once
  const user = await honcho.peer(userId);
  const assistant = await honcho.peer("assistant");
  const session = await honcho.session(sessionId);

  const result = await graph.invoke({
    userInput: userInput,
    user: user,
    assistant: assistant,
    session: session,
  });

  return result.assistantResponse;
}

// Interactive chat loop
async function main() {
  console.log("Welcome to the AI Assistant! How can I help you today?");
  const userId = "test-user-123";

  const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout,
  });

  while (true) {
    const userInput = await rl.question("You: ");
    if (userInput.toLowerCase() === "quit" || userInput.toLowerCase() === "exit") {
      rl.close();
      break;
    }
    const response = await runConversation(userId, userInput);
    console.log(`Assistant: ${response}\n`);
  }
}

main();
```
</CodeGroup>

## Next Steps

Now that you have a working LangGraph integration with Honcho, you can:

- **Create tools with Honcho** to fully utilize the Honcho's memory & context management features
- **Augment multi-agent LangGraphs** with multiple assistant peers

## Related Resources

<CardGroup cols={2}>
  <Card title="Get Context" icon="messages" href="/v2/documentation/core-concepts/features/get-context">
    Learn more about retrieving and formatting conversation context
  </Card>
  <Card title="MCP Integration" icon="star-of-life" href="/v2/integrations/mcp">
    Use Honcho in Claude Desktop with MCP
  </Card>
</CardGroup>
