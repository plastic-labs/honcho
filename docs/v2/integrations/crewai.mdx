---
title: "CrewAI"
icon: 'users-gear'
description: "Build AI agents with persistent memory using CrewAI and Honcho"
sidebarTitle: 'CrewAI'
---

Integrate Honcho with CrewAI to build AI agents that maintain memory across sessions. This guide shows you how to use Honcho's memory layer with CrewAI's agent orchestration framework.

<Note>
The full code is available on [GitHub](https://github.com/plastic-labs/honcho/tree/main/examples/crewai) with examples in [Python](https://github.com/plastic-labs/honcho/blob/main/examples/crewai/python/main.py)
</Note>

## What We're Building

We'll create AI agents that remember and reason over past conversations. Here's how the pieces fit together:

- **CrewAI** orchestrates agent behavior and task execution
- **Honcho** stores messages and retrieves relevant context
- **Your LLM** generates responses using Honcho's formatted context

The key benefit: CrewAI automatically retrieves relevant conversation history from Honcho without you needing to manually manage context, token limits, or message formatting.

<Note>
This tutorial demonstrates a single-agent setup to show how Honcho integrates with CrewAI. For production applications, you can extend this to multi-agent crews with shared or individual memory using Honcho's peer system.
</Note>

## Setup

Install required packages:

<CodeGroup>
```bash Python (uv)
uv add honcho_crewai crewai python-dotenv
```

```bash Python (pip)
pip install honcho_crewai crewai python-dotenv
```
</CodeGroup>

This tutorial uses OpenAI for the LLM, but Honcho works with any LLM provider. Create a `.env` file with your API keys:

```bash
OPENAI_API_KEY=your_openai_key
```

<Note>
This tutorial uses the Honcho demo server at https://demo.honcho.dev which runs a small instance of Honcho on the latest version. For production, get your Honcho API key at [app.honcho.dev](https://app.honcho.dev). For local development, use `environment="local"`.
</Note>

## Initialize Imports

```python Python
from typing import Optional
from dotenv import load_dotenv
from crewai import Agent, Task, Crew, Process
from crewai.memory.external.external_memory import ExternalMemory
from honcho_crewai import HonchoStorage

load_dotenv()
```

## Understanding Honcho Storage

The `honcho_crewai` package provides `HonchoStorage`, a storage provider that implements CrewAI's `Storage` interface using Honcho's session-based memory.

<Note>
Before proceeding, it's important to understand Honcho's core concepts (`Peers` and `Sessions`). Review the [Honcho Architecture](/v2/documentation/core-concepts/architecture) to familiarize yourself with these primitives.
</Note>

The `HonchoStorage` class implements three key methods:

- **`save()`** - Stores messages in Honcho's session, associating them with the appropriate peer (user or assistant)
- **`search()`** - Performs semantic vector search using `session.search()` to find messages most relevant to the query, and includes session summaries when available for context compression. This provides better relevance matching than simple retrieval.
- **`reset()`** - Creates a new session to start fresh conversations

CrewAI automatically calls these methods when agents need to store or retrieve memory, creating a seamless integration.

## Create Agents with Memory

Now let's create a function that initializes a CrewAI agent with Honcho-backed memory:

```python Python
def run_conversation_turn(
    user_id: str,
    user_input: str,
    session_id: Optional[str] = None,
    storage: Optional[HonchoStorage] = None
) -> tuple[str, HonchoStorage]:
    """
    Run a single conversation turn with the CrewAI agent.

    Args:
        user_id: Unique identifier for the user
        user_input: User's message
        session_id: Optional session ID for conversation continuity
        storage: Optional existing HonchoStorage instance

    Returns:
        Tuple of (agent_response, storage_instance)
    """
    # Initialize or reuse storage
    if storage is None:
        if not session_id:
            session_id = f"session_{user_id}"
        storage = HonchoStorage(user_id=user_id, session_id=session_id)

    # Create ExternalMemory wrapper for automatic context retrieval
    external_memory = ExternalMemory(storage=storage)

    # Save user input to memory
    external_memory.save(user_input, metadata={"agent": "user"})

    # Create an agent with memory
    agent = Agent(
        role="AI Assistant",
        goal="Help users with their questions and remember context from previous conversations",
        backstory=(
            "You are a helpful AI assistant with the ability to remember past conversations. "
            "You use context from previous interactions to provide personalized and relevant responses."
        ),
        verbose=False,
        allow_delegation=False
    )

    # Create task for the agent
    task = Task(
        description=f"Respond to the user's message: {user_input}",
        expected_output="A helpful and contextually relevant response that considers conversation history",
        agent=agent
    )

    # Create crew with external memory - enables automatic context retrieval
    crew = Crew(
        agents=[agent],
        tasks=[task],
        process=Process.sequential,
        external_memory=external_memory,
        verbose=False
    )

    # Execute - CrewAI automatically retrieves relevant context from Honcho
    result = crew.kickoff()

    # Save assistant response back to memory
    response_text = str(result.raw)
    external_memory.save(response_text, metadata={"agent": "assistant"})

    return response_text, storage
```

### How Memory Retrieval Works

When you pass `external_memory` to the `Crew`, CrewAI automatically:

1. Calls `storage.search()` to retrieve relevant context from Honcho
2. Injects this context into the agent's working memory
3. Uses the context when generating responses
4. Stores new messages back to Honcho via `storage.save()`

This happens transparently without manual memory management.

## Chat Loop

Create the main conversation function with an interactive loop:

<Note>
**Production Usage:** Honcho accepts any nanoid-compatible string for `user_id` and `session_id`. You can use IDs directly from your authentication system (Auth0, Firebase, Clerk, etc.) and session management without modification.

This tutorial uses hardcoded values for simplicity.
</Note>

```python Python
def main():
    """Interactive chat loop with CrewAI agent powered by Honcho memory."""
    print("Welcome to the AI Assistant powered by CrewAI and Honcho!")
    print("Type 'quit' or 'exit' to end the conversation.\\n")

    user_id = "demo-user-123"
    storage = None

    while True:
        user_input = input("You: ")
        if user_input.lower() in ['quit', 'exit']:
            print("Goodbye!")
            break

        if not user_input.strip():
            continue

        try:
            response, storage = run_conversation_turn(
                user_id=user_id,
                user_input=user_input,
                storage=storage
            )
            print(f"Assistant: {response}\\n")
        except Exception as e:
            print(f"Error: {e}\\n")


if __name__ == "__main__":
    main()
```

## Next Steps

Now that you have a working CrewAI integration with Honcho, you can:

- **Build multi-agent crews** where each agent is a Honcho `Peer` with its own memory
- **Create specialized agents** with domain-specific memory and context
- **Use CrewAI's advanced features** like hierarchical processes, tool delegation, and conditional task execution

## Related Resources

<CardGroup cols={2}>
  <Card title="Get Context" icon="messages" href="/v2/documentation/core-concepts/features/get-context">
    Learn more about retrieving and formatting conversation context
  </Card>
  <Card title="LangGraph Integration" icon="diagram-project" href="/v2/integrations/langgraph">
    Build stateful agents with LangGraph and Honcho
  </Card>
</CardGroup>
