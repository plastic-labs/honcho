---
title: 'Get Context'
description: 'Learn how to use get_context() to retrieve and format conversation context for LLM integration'
icon: 'messages'
---

The `get_context()` method is a powerful feature that retrieves formatted conversation context from sessions, making it easy to integrate with LLMs like OpenAI, Anthropic, and others. This guide covers everything you need to know about working with session context.

<Note>
By default, the context includes a blend of summary and messages ***which covers the entire session history of a peer***.
</Note>

Summaries are automatically generated at intervals and recent messages are included depending on how many tokens the context is intended to be. You can specify any token limit you want, and can disable summaries to fill that limit entirely with recent messages. To get representation data, you need to specify a target peer.

## Basic Usage

The `get_context()` method is available on all Session objects and returns a `SessionContext` that contains the formatted conversation history.

<CodeGroup>
```python Python
from honcho import Honcho

# Initialize client and create session
honcho = Honcho()
session = honcho.session("conversation-1")

# Get basic context (not very useful before adding any messages!)
context = session.get_context()
```

```typescript TypeScript
import { Honcho } from "@honcho-ai/sdk";

(async () => {
    // Initialize client and create session
    const honcho = new Honcho({});
    const session = await honcho.session("conversation-1");

    // Get basic context (not very useful before adding any messages!)
    const context = await session.getContext();
})();
```
</CodeGroup>

## Context Parameters

The `get_context()` method accepts several optional parameters to customize the retrieved context:

### Token Limits

Control the size of the context by setting a maximum token count:

<CodeGroup>
```python Python
# Limit context to 1500 tokens
context = session.get_context(tokens=1500)

# Limit context to 3000 tokens for larger conversations
context = session.get_context(tokens=3000)
```

```typescript TypeScript
(async () => {
    // Limit context to 1500 tokens
    const context = await session.getContext({ tokens: 1500 });

    // Limit context to 3000 tokens for larger conversations
    const context = await session.getContext({ tokens: 3000 });
})();
```
</CodeGroup>

### Summary Mode

Enable summary mode (on by default) to get a condensed version of the conversation:

<CodeGroup>
```python Python
# Get context with summary enabled -- will contain both summary and messages
context = session.get_context(summary=True)

# Combine summary=False with token limits to get more messages
context = session.get_context(summary=False, tokens=2000)
```

```typescript TypeScript
(async () => {
    // Get context with summary enabled -- will contain both summary and messages
    const context = await session.getContext({ summary: true });

    // Combine summary=False with token limits to get more messages
    const context = await session.getContext({
      summary: false,
      tokens: 2000
    });
})();
```
</CodeGroup>

### Peer Representation in Context

You can include a peer's [representation](/v2.6.0-alpha/documentation/core-concepts/representation) and peer card in the context by specifying `peer_target`. This is useful for providing the LLM with knowledge about a specific peer.

<CodeGroup>
```python Python
# Get context with peer representation included
context = session.get_context(
    tokens=2000,
    peer_target="user-123"  # Include representation of user-123
)

# Access the representation and peer card
print(context.peer_representation)  # String representation
print(context.peer_card)            # List of peer card items

# Get representation from a specific peer's perspective
context = session.get_context(
    tokens=2000,
    peer_target="user-123",
    peer_perspective="assistant"  # From assistant's viewpoint
)
```

```typescript TypeScript
(async () => {
    // Get context with peer representation included
    const context = await session.getContext({
      tokens: 2000,
      peerTarget: "user-123"  // Include representation of user-123
    });

    // Access the representation and peer card
    console.log(context.peerRepresentation);  // String representation
    console.log(context.peerCard);            // Array of peer card items

    // Get representation from a specific peer's perspective
    const perspectiveContext = await session.getContext({
      tokens: 2000,
      peerTarget: "user-123",
      peerPerspective: "assistant"  // From assistant's viewpoint
    });
})();
```
</CodeGroup>

### Semantic Search with Last Message

Use `last_user_message` to fetch semantically relevant conclusions based on the most recent message (requires `peer_target`):

<CodeGroup>
```python Python
context = session.get_context(
    tokens=2000,
    peer_target="user-123",
    last_user_message="What are my coding preferences?",
    search_top_k=10,           # Number of relevant observations
    search_max_distance=0.8,   # Max semantic distance (0.0-1.0)
    include_most_derived=True, # Include most recent observations
    max_observations=25        # Cap total observations
)
```

```typescript TypeScript
(async () => {
    const context = await session.getContext({
      tokens: 2000,
      peerTarget: "user-123",
      lastUserMessage: "What are my coding preferences?",
      representationOptions: {
        searchTopK: 10,           // Number of relevant observations
        searchMaxDistance: 0.8,   // Max semantic distance (0.0-1.0)
        includeMostDerived: true, // Include most recent observations
        maxObservations: 25       // Cap total observations
      }
    });
})();
```
</CodeGroup>

### Session-Scoped Representations

Use `limit_to_session` to only include observations from the current session:

<CodeGroup>
```python Python
# Get context limited to this session's observations only
context = session.get_context(
    tokens=2000,
    peer_target="user-123",
    limit_to_session=True  # Only observations from this session
)
```

```typescript TypeScript
(async () => {
    // Get context limited to this session's observations only
    const context = await session.getContext({
      tokens: 2000,
      peerTarget: "user-123",
      limitToSession: true  // Only observations from this session
    });
})();
```
</CodeGroup>

### All Parameters Reference

| Parameter | Type | Description |
|-----------|------|-------------|
| `summary` | `bool` | Include summary in context (default: true) |
| `tokens` | `int` | Maximum tokens to include |
| `peer_target` | `str` | Peer ID to include representation for |
| `peer_perspective` | `str` | Peer ID for perspective (requires peer_target) |
| `last_user_message` | `str` | Message for semantic search (requires peer_target) |
| `limit_to_session` | `bool` | Limit to session observations only |
| `search_top_k` | `int` | Semantic search results to include (1-100) |
| `search_max_distance` | `float` | Max semantic distance (0.0-1.0) |
| `include_most_derived` | `bool` | Include most recently derived observations |
| `max_observations` | `int` | Maximum observations to include (1-100) |

## Converting to LLM Formats

The `SessionContext` object provides methods to convert the context into formats compatible with popular LLM APIs. When converting to OpenAI format, you must specify the assistant peer to format the context in such a way that the LLM can understand it.

### OpenAI Format

Convert context to OpenAI's chat completion format:

<CodeGroup>
```python Python
# Create peers
alice = honcho.peer("alice")
assistant = honcho.peer("assistant")

# Add some conversation
session.add_messages([
    alice.message("What's the weather like today?"),
    assistant.message("It's sunny and 75째F outside!")
])

# Get context and convert to OpenAI format
context = session.get_context()
openai_messages = context.to_openai(assistant=assistant)

# The messages are now ready for OpenAI API
print(openai_messages)
# [
#   {"role": "user", "content": "What's the weather like today?"},
#   {"role": "assistant", "content": "It's sunny and 75째F outside!"}
# ]
```

```typescript TypeScript
(async () => {
    // Create peers
    const alice = await honcho.peer("alice");
    const assistant = await honcho.peer("assistant");

    // Add some conversation
    await session.addMessages([
      alice.message("What's the weather like today?"),
      assistant.message("It's sunny and 75째F outside!")
    ]);

    // Get context and convert to OpenAI format
    const context = await session.getContext();
    const openaiMessages = context.toOpenAI(assistant);

    // The messages are now ready for OpenAI API
    console.log(openaiMessages);
    // [
    //   {"role": "user", "content": "What's the weather like today?"},
    //   {"role": "assistant", "content": "It's sunny and 75째F outside!"}
    // ]
})();
```
</CodeGroup>

### Anthropic Format

Convert context to Anthropic's Claude format:

<CodeGroup>
```python Python
# Get context and convert to Anthropic format
context = session.get_context()
anthropic_messages = context.to_anthropic(assistant=assistant)

# Ready for Anthropic API
print(anthropic_messages)
```

```typescript TypeScript
(async () => {
    // Get context and convert to Anthropic format
    const context = await session.getContext();
    const anthropicMessages = context.toAnthropic(assistant);

    // Ready for Anthropic API
    console.log(anthropicMessages);
})();
```
</CodeGroup>

## Complete LLM Integration Examples

### Using with OpenAI

<CodeGroup>
```python Python
import openai
from honcho import Honcho

# Initialize clients
honcho = Honcho()
openai_client = openai.OpenAI()

# Set up conversation
session = honcho.session("support-chat")
user = honcho.peer("user-123")
assistant = honcho.peer("support-bot")

# Add conversation history
session.add_messages([
    user.message("I'm having trouble with my account login"),
    assistant.message("I can help you with that. What error message are you seeing?"),
    user.message("It says 'Invalid credentials' but I'm sure my password is correct")
])

# Get context for LLM
messages = session.get_context(tokens=2000).to_openai(assistant=assistant)

# Add new user message and get AI response
messages.append({
    "role": "user",
    "content": "Can you reset my password?"
})

response = openai_client.chat.completions.create(
    model="gpt-4",
    messages=messages
)

# Add AI response back to session
session.add_messages([
    user.message("Can you reset my password?"),
    assistant.message(response.choices[0].message.content)
])
```

```typescript TypeScript
import OpenAI from 'openai';
import { Honcho } from "@honcho-ai/sdk";

(async () => {
    // Initialize clients
    const honcho = new Honcho({});
    const openai = new OpenAI();

    // Set up conversation
    const session = await honcho.session("support-chat");
    const user = await honcho.peer("user-123");
    const assistant = await honcho.peer("support-bot");

    // Add conversation history
    await session.addMessages([
      user.message("I'm having trouble with my account login"),
      assistant.message("I can help you with that. What error message are you seeing?"),
      user.message("It says 'Invalid credentials' but I'm sure my password is correct")
    ]);

    // Get context for LLM
    const messages = await session.getContext({ tokens: 2000 }).toOpenAI(assistant);

    // Add new user message and get AI response
    const response = await openai.chat.completions.create({
      model: "gpt-4",
      messages: [
        ...messages,
        { role: "user", content: "Can you reset my password?" }
      ]
    });

    // Add AI response back to session
    await session.addMessages([
      user.message("Can you reset my password?"),
      assistant.message(response.choices[0].message.content)
    ]);
})();
```
</CodeGroup>

### Multi-Turn Conversation Loop

<CodeGroup>
```python Python
def chat_loop():
    """Example of a continuous chat loop using get_context()"""

    session = honcho.session("chat-session")
    user = honcho.peer("user")
    assistant = honcho.peer("ai-assistant")

    while True:
        # Get user input
        user_input = input("You: ")
        if user_input.lower() in ['quit', 'exit']:
            break

        # Add user message to session
        session.add_messages([user.message(user_input)])

        # Get conversation context
        context = session.get_context(tokens=2000)
        messages = context.to_openai(assistant=assistant)

        # Get AI response
        response = openai_client.chat.completions.create(
            model="gpt-4",
            messages=messages
        )

        ai_response = response.choices[0].message.content
        print(f"Assistant: {ai_response}")

        # Add AI response to session
        session.add_messages([assistant.message(ai_response)])

# Start the chat loop
chat_loop()
```

```typescript TypeScript
(async () => {
    async function chatLoop() {
      const session = await honcho.session("chat-session");
      const user = await honcho.peer("user");
      const assistant = await honcho.peer("ai-assistant");

      // This would be replaced with actual user input handling in a real app
      const userInputs = [
        "Hello, how are you?",
        "What's the weather like?",
        "Tell me a joke"
      ];

      for (const userInput of userInputs) {
        console.log(`You: ${userInput}`);

        // Add user message to session
        await session.addMessages([user.message(userInput)]);

        // Get conversation context
        const context = await session.getContext({ tokens: 2000 });
        const messages = context.toOpenAI(assistant);

        // Get AI response
        const response = await openai.chat.completions.create({
          model: "gpt-4",
          messages: messages
        });

        const aiResponse = response.choices[0].message.content;
        console.log(`Assistant: ${aiResponse}`);

        // Add AI response to session
        await session.addMessages([assistant.message(aiResponse)]);
      }
    }

    // Start the chat loop
    await chatLoop();
})();
```
</CodeGroup>

## Advanced Context Usage

### Context with Summaries for Long Conversations

For very long conversations, use summaries to maintain context while controlling token usage:

<CodeGroup>
```python Python
# For long conversations, use summary mode
long_session = honcho.session("long-conversation")

# Get summarized context to fit within token limits
context = long_session.get_context(summary=True, tokens=1500)
messages = context.to_openai(assistant=assistant)

# This will include a summary of older messages and recent full messages
print(f"Context contains {len(messages)} formatted messages")
```

```typescript TypeScript
(async () => {
    // For long conversations, use summary mode
    const longSession = await honcho.session("long-conversation");

    // Get summarized context to fit within token limits
    const context = await longSession.getContext({
      summary: true,
      tokens: 1500
    });
    const messages = context.toOpenAI(assistant);

    // This will include a summary of older messages and recent full messages
    console.log(`Context contains ${messages.length} formatted messages`);
})();
```
</CodeGroup>

### Context for Different Assistant Types

You can get context formatted for different types of assistants in the same session:

<CodeGroup>
```python Python
# Create different assistant peers
chatbot = honcho.peer("chatbot")
analyzer = honcho.peer("data-analyzer")
moderator = honcho.peer("moderator")

# Get context formatted for each assistant type
chatbot_context = session.get_context().to_openai(assistant=chatbot)
analyzer_context = session.get_context().to_openai(assistant=analyzer)
moderator_context = session.get_context().to_openai(assistant=moderator)

# Each context will format the conversation from that assistant's perspective
```

```typescript TypeScript
(async () => {
    // Create different assistant peers
    const chatbot = await honcho.peer("chatbot");
    const analyzer = await honcho.peer("data-analyzer");
    const moderator = await honcho.peer("moderator");

    // Get context formatted for each assistant type
    const context = await session.getContext();
    const chatbotContext = context.toOpenAI(chatbot);
    const analyzerContext = context.toOpenAI(analyzer);
    const moderatorContext = context.toOpenAI(moderator);

    // Each context will format the conversation from that assistant's perspective
})();
```
</CodeGroup>

## Best Practices

### 1. Token Management

Always set appropriate token limits to control costs and ensure context fits within LLM limits:

<CodeGroup>
```python Python
# Good: Set reasonable token limits based on your model
context = session.get_context(tokens=3000)  # For GPT-4
context = session.get_context(tokens=1500)  # For smaller models

# Good: Use summaries for very long conversations
context = session.get_context(summary=True, tokens=2000)
```

```typescript TypeScript
(async () => {
    // Good: Set reasonable token limits based on your model
    const context = await session.getContext({ tokens: 3000 });  // For GPT-4
    const context = await session.getContext({ tokens: 1500 });  // For smaller models

    // Good: Use summaries for very long conversations
    const context = await session.getContext({ summary: true, tokens: 2000 });
})();
```
</CodeGroup>

### 2. Context Caching

For applications with frequent context retrieval, consider caching context when appropriate:

<CodeGroup>
```python Python
# Cache context for multiple LLM calls within the same request
context = session.get_context(tokens=2000)
openai_messages = context.to_openai(assistant=assistant)
anthropic_messages = context.to_anthropic(assistant=assistant)

# Use the same context object for multiple format conversions
```

```typescript TypeScript
(async () => {
    // Cache context for multiple LLM calls within the same request
    const context = await session.getContext({ tokens: 2000 });
    const openaiMessages = context.toOpenAI(assistant);
    const anthropicMessages = context.toAnthropic(assistant);

    // Use the same context object for multiple format conversions
})();
```
</CodeGroup>

### 3. Error Handling

Always handle potential errors when retrieving context:

<CodeGroup>
```python Python
try:
    context = session.get_context(tokens=2000)
except Exception as e:
    print(f"Error getting context: {e}")
    # Handle error appropriately (fallback to basic context, retry, etc.)
```

```typescript TypeScript
(async () => {
    try {
      const context = await session.getContext({ tokens: 2000 });
    } catch (error) {
      console.error(`Error getting context: ${error}`);
      // Handle error appropriately (fallback to basic context, retry, etc.)
    }
})();
```
</CodeGroup>

## Conclusion

The `get_context()` method is essential for integrating Honcho sessions with LLMs. By understanding how to:

- Retrieve context with appropriate parameters
- Convert context to LLM-specific formats
- Manage token limits and summaries
- Handle multi-turn conversations

You can build sophisticated AI applications that maintain conversation history and context across interactions while integrating seamlessly with popular LLM providers.
