---
title: 'LangChain Integration ü¶ú‚õìÔ∏è' 
sidebarTitle: 'LangChain'
description: 'Using Honcho with LangChain with drop-in primitives'
icon: 'bird'
---

You can use Honcho to manage user context around LLM frameworks like LangChain. First, import the appropriate packages:

```python
from honcho import Honcho

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import AIMessage, HumanMessage

from dotenv import load_dotenv   # for loading in LLM API keys

load_dotenv()   # assumes you have a .env file with OPENAI_API_KEY defined
```

Next let's instantiate our Honcho client:

```python
honcho = Honcho(environment="demo")
app_name = "LangChain App"
app = honcho.apps.get_or_create(name=app_name) # create or get app

user_name = str(uuid4())
user = honcho.apps.users.get_or_create(app_id=app.id, name=user_name) # create or get user
```

Then we can define our chain using the LangChain Expression Language ([LCEL](https://python.langchain.com/docs/expression_language/why)):
```python
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{input}")
])
model = ChatOpenAI(model="gpt-3.5-turbo")
output_parser = StrOutputParser()

chain = prompt | model | output_parser
```

Honcho returns lists of `Message` objects when queried using a built-in method like `get_messages()`, so a quick utility function is needed to change the list format to message objects LangChain expects:

```python
def messages_to_langchain(messages: List):
    new_messages = []
    for message in messages:
        if message.is_user:
            new_messages.append(HumanMessage(content=message.content))
        else:
            new_messages.append(AIMessage(content=message.content))
    return new_messages
```

This method is importable with the following statement

```python
from honcho.lib.ext.langchain import messages_to_langchain
```

Now we can structure Honcho calls around our LLM inference:
```python
sessions = [
    session for session in 
    honcho.apps.users.sessions.list(
        app_id=app.id,
        user_id=user.id,
        location_id=location_id
    ) 
]  # args come from application logic
session = sessions[0]   # most recent session for user
history = [
    message for message in 
    honcho.apps.users.sessions.messages.list(
        app_id=app.id,
        user_id=user.id,
        session_id=session.id
    )
]
chat_history = messages_to_langchain(history)   # convert messages for LangChain
inp = "Here's a user message!"
honcho.apps.users.sessions.messages.create(app_id=app.id, user_id=user.id, is_user=True, content=inp)

response = await chain.ainvoke({"chat_history": chat_history, "input": inp})

honcho.apps.users.sessions.messages.create(app_id=app.id, user_id=user.id, is_user=False, content=response)
```

Here we query messages from a user's session using Honcho and construct a chat history object to send to the LLM alongside our immediate user input. Once the LLM has responded, we can add that to Honcho!
