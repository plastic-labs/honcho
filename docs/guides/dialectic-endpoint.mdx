---
title: "Dialectic Endpoint"
description: "An endpoint for reasoning about your users"
icon: "comments"
---

Honcho by default runs ambient inference on top of the `message` objects you store. Those messages serve as the ground truth upon which facts about the user are derived and stored. The **Dialectic Endpoint** is the natural language interface through which insights are synthesized from those facts. We believe [intellectual respect](https://blog.plasticlabs.ai/extrusions/Extrusion-02.24) for LLMs is paramount in building effective AI agents/apps. It follows that the LLM should know better than any human what would aid them in their generation task. Thus, the Dialectic endpoint exists for flexible agent-to-agent communication. 

## Automatic Fact Derivation

On every message written to a session, an automatic callback is run that will reason about the conversation and store facts in a `collection` named `honcho`. This is a reserved `collection` specifically for the backend Honcho agent to interact with. 

## Dialectic Endpoint

The Dialectic endpoint allows you to define logic enabling your agent to talk to our agent that automatically retrieves and synthesizes facts from the collection. You can use the response as part of your reasoning process for your agentâ€“add it to your next prompt to inject critical context about the user.

This chat interface is exposed via the `peer.chat()` endpoint. It accepts a string or a list of strings. Below is some example code on how this works. 

## Prerequisites

<CodeGroup>
```python Python
from honcho import Honcho

# use the default workspace
honcho = Honcho()

# get/create a peer
peer = honcho.peer("demo-user")

# get/create a session
session = honcho.session("demo-session")

# (assuming some messages have been written to Honcho for the deriver to use)
```

```typescript TypeScript
import Honcho from '@honcho-ai/sdk';

// use the default workspace
const honcho = new Honcho();

// get/create a peer
const peer = await honcho.peer('demo-user');

// get/create a session
const session = await honcho.session('demo-session');

// (assuming some messages have been written to Honcho for the deriver to use)
```
</CodeGroup>
## Static Dialectic Call 

<CodeGroup>
```python Python
query = "What is the user's favorite way of completing the task?"
answer = peer.chat(query)
```

```typescript TypeScript
const query = "What is the user's favorite way of completing the task?"
const dialecticResponse = await peer.chat(query)
```
</CodeGroup>

## Streaming Dialectic Call

<CodeGroup>
```python Python
query = "What do we know about the user?"
response_stream = peer.chat(query, stream=True)

for line in response_stream.iter_text():
    print(line)
```

```typescript TypeScript
const query = "What do we know about the user?"
const responseStream = await peer.chat(query, { stream: true })

for await (const line of responseStream.iter_text()) {
    console.log(line)
}
```
</CodeGroup>

We've designed the Dialectic endpoint to be infinitely flexible. We wrote an incomplete list of ideas on how to use it on our blog [here](https://blog.plasticlabs.ai/blog/Introducing-Honcho's-Dialectic-API#how-it-works).



